---
output:
  html_document: default
  pdf_document: default
---
<!-- ---
title: 'Statistical Lab 3 (Transformation to Normality and PCA R) '
author: "Jiwan Bhandari"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: no
vignette: |
%\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---
-->
<style>
#rowtables table{
  width:100%;
  max-width:100%;
  margin-bottom:40px;
}
</style>


<div style="text-align:center;min-height:29.7cm;width:21cm;vertical-align:center;">
<h4 style="">STAT 755 Lab Report 4</h4>
<h4>Jiwan Bhandari</h4>
</div>
<br/><br/><br/><br/><br/>
```{r setup, include=FALSE}
library(htmlTable)
library(car)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library("mat2tex")
SA <- function(X,add=FALSE,data.plot=TRUE,...)
{
# Vector of means
#==============================
n<-dim(X)[1]
ones<-matrix(rep(1,n),ncol=1)
mu<-as.vector(t(X) %*% ones / n)
# Variance
#===========================================================
Sigma<-var(X)
e<-eigen(Sigma)
ellipse(mu,Sigma,3,add=add,xlim=range(X),ylim=range(X),...)
ellipse(mu,Sigma,2,add=TRUE)
ellipse(mu,Sigma,1,add=TRUE)
if (data.plot)
points(X[,1],X[,2],pch=20,col=4)
arrows(mu[1],mu[2],mu[1]+e$vectors[1,1]*sqrt(e$values[1]),
mu[2]+e$vectors[2,1]*sqrt(e$values[1]),length=.1,col='green',lwd=2)
arrows(mu[1],mu[2],mu[1]+e$vectors[1,2]*sqrt(e$values[2]),
mu[2]+e$vectors[2,2]*sqrt(e$values[2]),length=.1,col='green',lwd=2)

}

```
#### Introduction
Factor Analaysis like principal component analysis is an attempt to approximate the covariance matrix $\Sigma.$ The factor model postulates that $X$ is linearly dependent upon a few observable random variables $F_1,F_2,.....F_m$ called common factors and p additional sources of variations $\epsilon_1\epsilon_2,.....\epsilon_p$
called errors or sometimes specific factors. In particular, the factor analysis model is $$\underset{p*1}{X-\mu} = \underset{p*m}{L} \underset{m*1}{F} + \underset{p*1}{\epsilon}$$,
where $L$ is the matrix of factor loadings or matrix of coefficients of linear transformation, $F$ is matrix of common Factors. Constraining this linear relation such that $E(F) = 0$, $Cov(F)=I$ , $E(\epsilon)=0$ and $Cov(\epsilon)=\Psi$ where $\Psi$ is a diagonal matrix, we get a factor model called orthogonal factor model. Imposing and applying these constraints on the preceeding linear transformation equation, gives a new linear equation that allows us to express the $\Sigma$, the variance-covariance matrix in terms of loadings and errors. $$\Sigma=LL' +\Psi$$

#### Dataset
The dataset  (n= 50, p=7) used in this exercise is called salesperson dataset. There are 7 variables. The first 3 measure the performance of the sales staffs : growth of sales, profitability of sales, and new-acccount sales. These measure have been converted to scale, on which 100 indicates "avergae" performance. The other 4 variables are test scores which are purported to measure creativity, mechanical reasoning, abstract reasoning, and mathematical ability respectively.
```{r echo =FALSE}
   D <- read.table("./salespeople.txt",header=FALSE,sep="")
   D_Names <- c("Sales_Growth","Profitability","Account_Sales","Creativity_test","Mech_Reasoning_Test","Abs_reasoning_test","Math_test")
   A_Names <- c("Sales Growth","Profitability of sales"," New Account Sales", " Creativity Test"," Mechanical Reasoning Test"," Abstract Reasoning Test"," Mathematical Test")
   names(D)<-D_Names
```

#### Correlation Matrix
The units of the variables in the dataset aren't exactly commensurate and the variances differ significantly.  In order to avoid few variables with large variance unduly influencing the determination of factor loading it's desirable to work with scaled variances or correlations coefficients. The correlation matrix of the dataset is presented in the following table.
```{r echo = FALSE}
    knitr::kable(round(cor(D),3))
```

The above table clearly indicates that $Sales\ Growth ,\ Profitability\ ,\ Account\ Sales, \ \& \  Mathematical \ test$ form a group. As for the other variables, they don't indicate as clear grouping.

#### Normality Test
Normality data is a very important property of the data. In factor analysis normality of the underlying data allows us to estimate the loadings and factor using maximum likelihood estimation. Below we test normality of the data.  First, we test the univariate normality using quantile-quantile plot.  

```{r echo = FALSE, fig.align='center', fig.width=10, fig.height=4}
  i<-1
  par(mfrow=c(1,2))
  p <- apply(D,2,function(x){
    qqnorm(x,main=A_Names[i])
    qqline(x)
    i<<-i+1
  })

```
From the qqplots, it evident that $profitability\ of\ sales,\ \& Mathematical\ Test$ show normality. $New\ Acount Sales \ $ is also more or less normal. Rest of the variables don't show normality.

Next, we test multivariate normality of the dataset using Kolmogorow-Simrnow test. Specifically, we test whether the statistical distances have chi-square distribution.

```{r echo=TRUE}
   D <- as.matrix(D)
   df <- ncol(D)
   cov_mat <- cov(D)
   Sigma_inv <- solve(cov_mat)
   DS <- sweep(D,2,colMeans(D))
   d = rowSums(DS%*%Sigma_inv*DS)
   testResult <- ks.test(d,"pchisq",df)

```

$H_0:$ Statistical distances have chi-square distribution.<br/>
$H_1:$ Statistical distances don't have chi-square distribution.<br/>
$Decision \ Rule:$ Reject $H_0$ if $p-value < \alpha (0.05)$ otherwise don't reject $H_0$.<br/>
$Test\ Statistics :$  $p-value = `r round(testResult$p.value,2)` $.<br/>
$Conclusion:$ Since `r round(testResult$p.value,2)` $>0.05$ and in-fact p-value in this case greater than any common level of $\alpha$'s we fail to reject the null hypothesis. Thus, the data does have chi-square distribution meaning the underlying data does have normal distribution.<br/>

 
#### Factor Analysis

In factor analysis, the number of common factor is usually determined by a priori considerations. However, if it's not possible to determine the number of factors by domain knowledge, the choice of factors can be based on the estimated eigen values in much the same manner as with principal component analysis. The frequently encountered apporach is to choose the number of common factors to be equal to the numberof eighen values of correlation matrix greater than 1. This is usually a rule of thumb. It's always a good idea to iteratively experiment several values and pick the best one. For our dataset we try 3 choices (m = 1,2,3) of common factors as R software doesn't allow more than 3 factors with 7 variables. <br/>
The estimated factor loadings, communalities, specific variances and proportion of total sample variance explained by each factor for $m = 1,2,3$ factor solutions are shown in the following 3 tables. The tables show the factor solutions using 3 factor rotations  - $'none',\  'varimax',\ 'promax'$ alongside one another.

<div id='rowtables'>

```{r echo = FALSE,results="asis"}
  m <- c(1:3)
  for(m_i in m){
      no_fa <- factanal(x=as.matrix(D),factors = m_i,scores = "Bart",rotation = "none")
      var_fa<-factanal(x=as.matrix(D),factors = m_i, scores = "Bart", rotation = "varimax")
      pro_fa <- factanal(x=as.matrix(D),factors = m_i,scores = "Bart",rotation = "promax")
      
      output <- cbind(no_fa$loadings,var_fa$loadings,pro_fa$loadings)
      output <- cbind(output,1-no_fa$uniqueness,no_fa$uniqueness)
      e <- colSums(output^2)[1:(m_i*3)]
      e <- c(e,rep(NA,dim(output)[2]-length(e)))
      
      cumvar=lapply(split(e,as.numeric(gl(length(e),m_i,length(e)))),cumsum) 
      output <- rbind(output,unlist(cumvar)/7)
      output[abs(output[,1: (m_i*3)])<0.1] <-NA
      output <- round(output,2)

      
      r_n <- c(A_Names,"Commulative Variance")
      n_facs <- m_i
      c1 <- n_facs *3 + 2 
    
      c_groups <- cbind( paste("Factor Loadings (",m_i,"-factor solution)")," &nbsp;",matrix(rep(NA,c1-2),nrow=1))
      c_gr <- cbind("Unrotated","Rot.(varimax)","Rot.(promax)","Communalities ","Uniqueness")
      c_gr <- cbind(c_gr,matrix(rep(NA,c1-dim(c_gr)[2]),nrow=1))
      c_groups <- rbind(c_groups,c_gr)
    
      h_names <- paste(c("F"),"<sub>",c(1:n_facs),"</sub>",sep="")
      header_names <- c(rep(h_names,3),NA,NA)
      colnames(output) <-header_names
    
      ncgroup <- cbind(n_facs*3,c1-(n_facs*3),matrix(rep(NA,c1-2),nrow=1))
      
      ncgr <- cbind(matrix(rep(n_facs,3),nrow=1),1,1)
      ncgr <- cbind(ncgr,matrix(rep(NA,c1-dim(ncgr)[2]),nrow=1))
      ncgroup <- rbind(ncgroup,ncgr)
      
      print(htmlTable(output,
            rowlabel= "Variables",
            rnames   = r_n,
            cgroup   = c_groups,
            n.cgroup= ncgroup ,
            total = TRUE))
      # res_no <- cor(D)- (no_fa$loadings %*% t(no_fa$loadings) + diag(no_fa$uniquenesses))
      # res_var<- cor(D) - (var_fa$loadings %*% t(var_fa$loadings) + diag(var_fa$uniquenesses))
      # res_pro<- cor(D) - (pro_fa$loadings %*% t(pro_fa$loadings) + diag(pro_fa$uniquenesses))
      # print(paste(m_i,"res_no",sum(res_no>0.01)))
      # print(paste(m_i,"res_no",sum(res_var>0.01)))
      # print(paste(m_i,"res_no",sum(res_pro>0.01)))
  } 
```
</div>

##### Residuals
We calcualted the residuals matrix for each value of common factor ($m=1 \ to \ 3$) using the following.
```{r echo=TRUE}
   residual <- cor(D) - (var_fa$loading %*% t(var_fa$loadings) + diag(var_fa$uniquenesses))
```
Upon calcuating the residual matrices for each value across each rotation, we filtered the residual matrices as $(R-LL'-\Psi)>0.01$. The result of this filtering showed, for each ($1,2,3$) choice  of $m$, $no-rotation$ and $varimax$ rotation had same and lower values but $promax$ rotation had bigger values. We got the best value with $m=3$ and $rotation = none \ or \ varimax.$  

##### Cummulative variance
We noticed from the tables above that the cumulative propotion of the total (standarized) sample variance increases as we increase the number of factors. For $m=1$, the commulative variance across each rotation type is $0.69$. For $m=2$ the cummulative variance for each rotation type is $\sim0.8$. For $m=3$, for $promax\  rotation$ we have cummulative variance of $0.94$ and for other rotation type it is $0.91$. So, clearly the proportion of the total variance explained by the 3-factor solution is appreicably larger than 2 and 1 factor solutions. 

##### Communalities
The communalities indicate the percentage of sample variances of each variables accounted for by the factors. In general, for each value ($1,2,3$) of common factor, we have high communalities. However, the communalities do increase with $m$. For instance, for $m=1$, the communality of variable $Creativity\  test$ is $0.32$ but for $m=2$ the communality goes to $1$ meaning that with $2$ factors we captured almost all the variance in $Creativity\  test$. Overall, with $ m=3$ we have best/highest communalities values.

##### Uniqueness
Uniqueness are the specific variances or the error variances. For good fit of factor model it's desirable to have low uniqueness values. From the tables above, we observe that the uniqueness values decrese as we increase the number of common factors. $m=3$ has the lowest uniqueness values meaning that factors have captured most of variablility.

##### Analysing loadings 
For **m=1** i.e 1-factor solution,  factor loadings are unaffected by rotation. Variables $Sales\ Growth$, $Profitability \ of \ sales$, $New \ Account \ Sales$, $Mathematical \ Test$ have high factor loadings on $F_1$ while $Creativity \ Test$, $Mechanical \ Reasoning\ Test$ and $Abstract\ Reasoning\ Test$ have relatively low loadings on $F_1$. The communalities values and commulative variance indicate that we do need additional factors to capture the variability of certain variables. <br/>

For **m=2** i.e 2-factor solution, for $unrotated\ factor\ loadings$ almost all variables load highly on the first factor except for $Abstract\ Reasoning \ Test$. The loadings don't indicate an apparent groupings with unrotated factor loadings. $Varimax \ \&\ promax$ rotations appear to crank up the factor loadings overall but more importantly they have injected more contrast in the loadings. From the rotated loadings, it's  more apparent that $sales\ growth$,$profitability\ of \ sales$ and $Mathematical\ Test$ load highly on $F_1$. It's also obvious that $Creativity\ test$ loads almost entirely on $F_2$. So, we clearly have $two\ groups$ and the remaining variables align highly towards $F_1$ than $F_2$. $F_1$ could be interpreted as $Quantitative\ performance \ factor$ because it captures the relationship between mathematical test score of a candidate and sales performance of the candidate.  $F_2$ could be interpreted as $creativity\ factor$ as it mostly dominated by $creativity\  test$. <br/>

For **m=3**,i.e. 3-factor solution $proxmax$ rotation clearly caputres more total standarized sample variance than $unroated$ and $varimax$ rotated loadings. With $3$ factors we do have variables that have no bearing on some of the factors. For $unrotated$ factors loadings there is very little contrast in $F_1$ to make out the groups. $F_2$ and $F_3$ do indicate groups with $F_2$ mostly influenced by $Profitability\ of\ sales$ and $Mathematical\ test$ while $F_3$ is mostly about $Creativity \ Test$ and $Abstract\ Resoning\ Test$. $F_3$ can also be interpretted as $factor\ of\ tests$ as it only captures test variables.
$Varimax$ rotated factor loadings have more contrast than $unrotated$ loadings. $F_1$ is dominated more by $Sales\ Growth, Profitability\ of\ Sales, Mathematical\ Test$, $F_2$ is mostly about the variance in $creativity test$ and $F_3$ is mostly about variance in $Abstract\ Reasoning\ Test$.
$Promax$ rotated loading give a slightly better picture of groups. $F_1$ is influenced by $Mathematical\ Test$, $Profitability\ of \ Sales$ and $Sales\ Growth$. $F_2$ and $F_3$ are mostly about $Creativity\ test$ and $Abstract\ Reasoning\ Test$. So, for $3-factor$ solution rotated factor loadings suggest $3 groups$.

##### Optimal Number of factors
Based on the above discussion on residuals, cummulative variances, communalities, uniqueness, and loadings analysis, I think it's warranted that $3$ factors are optimal rather than $2\ or\ 1$. $3$ factors, overall, give the maximum cummulative variance, lowest residuals, highest communalities. The amount of variance embodied by the final factor is $\sim20\%$ which implies that $F_3$ does infact account for significant amount variance. With $3$ factors we see more contrast in the loadings implying underlying variable groupings which simplifies the interpretation of loadings.  

##### Normality of Scores 
Below we test the normality of factor scores for 1-factor, 2-factor and 3-factor solutions using qqplots.

```{r echo =FALSE,fig.align="center", fig.width=10,fig.height=3}
      par(mfrow=c(1,3))
      n_fa <- factanal(x=as.matrix(D),factors = 1,scores = "Bart",rotation = "none")
      v_fa<-factanal(x=as.matrix(D),factors = 1, scores = "Bart", rotation = "varimax")
      p_fa <- factanal(x=as.matrix(D),factors = 1,scores = "Bart",rotation = "promax")
      
      qqnorm(n_fa$scores,main=paste("F1(rotation:none)"))
      qqline(n_fa$scores)
      qqnorm(v_fa$scores,main=paste("F1(rotation:varimax)"))
      qqline(v_fa$scores)
      qqnorm(p_fa$scores,main=paste("F1(rotation:promax)"))
      qqline(p_fa$scores)
```

```{r echo =FALSE,fig.align="center", fig.width=13,fig.height=3}
    f_fa <- list(factanal(x=as.matrix(D),factors = 2,scores = "Bart",rotation = "none"),
                factanal(x=as.matrix(D),factors = 2, scores = "Bart", rotation = "varimax"),
                factanal(x=as.matrix(D),factors = 2,scores = "Bart",rotation = "promax"))
    rotations <- c("none","varimax","promax")    
    par(mfrow=c(1,3*2))
    for(m_i in c(1:3)){
        ff_fa <- f_fa[[m_i]]
        maintext = paste(" (rot:",rotations[m_i],")",sep="")
        qqnorm(ff_fa$scores[,1],main=paste("F1",maintext,sep=""))
        qqline(ff_fa$scores[,1])
        qqnorm(ff_fa$scores[,2],main=paste("F2",maintext,sep=""))
        qqline(ff_fa$scores[,2])
    }
```

```{r echo =FALSE,fig.align="center", fig.width=16,fig.height=3}
    f_fa <- list(factanal(x=as.matrix(D),factors = 3,scores = "Bart",rotation = "none"),
                factanal(x=as.matrix(D),factors = 3, scores = "Bart", rotation = "varimax"),
                factanal(x=as.matrix(D),factors = 3,scores = "Bart",rotation = "promax"))
    rotations <- c("none","varimax","promax")    
    par(mfrow=c(1,3*3))
    for(m_i in c(1:3)){
        ff_fa <- f_fa[[m_i]]
        maintext = paste(" (rot:",rotations[m_i],")",sep="")
        qqnorm(ff_fa$scores[,1],main=paste("F1",maintext,sep=""))
        qqline(ff_fa$scores[,1])
        qqnorm(ff_fa$scores[,2],main=paste("F2",maintext,sep=""))
        qqline(ff_fa$scores[,2])
        qqnorm(ff_fa$scores[,3],main=paste("F3",maintext,sep=""))
        qqline(ff_fa$scores[,3])
    }
```

From the above qqplots, since the scores are linear and fall mainly in the $qqline$, we can infer that the scores are normally distributed. 

#####  FA with 2 common factors 
The second table above lists the factor loadings, communalities, specific variances and cummulative variances for the 2 factor solution using 'promax' rotation. Below, we plot the factor loadings for each rotation type.
```{r echo=FALSE,fig.align="center",fig.width=10,fig.height=4}
 
 no_fa<-factanal(x=as.matrix(D),factors = 2,scores = "Bart",rotation = "none")
 var_fa<-factanal(x=as.matrix(D),factors = 2,scores = "Bart",rotation = "varimax")
 pro_fa<-factanal(x=as.matrix(D),factors = 2,scores = "Bart",rotation = "promax")
 
 res_no <- cor(D)- (no_fa$loadings %*% t(no_fa$loadings) + diag(no_fa$uniquenesses))
 res_var<- cor(D) - (var_fa$loadings %*% t(var_fa$loadings) + diag(var_fa$uniquenesses))
 res_pro<- cor(D) - (pro_fa$loadings %*% t(pro_fa$loadings) + diag(pro_fa$uniquenesses))
 
 par(mfrow=c(1,3))
 xrange = c(-0.4,1.4)
 yrange = c(-0.4,1.2)
 # g <- ggplot()+geom_point(data=as.data.frame(no_fa),aes(x=Factor1,y=Factor2))
 
 plot(loadings(no_fa),pch=19,col='blue',xlab="Loadings for Factor 1", ylab="Loadings for Factor 2",   main="Factor Loadings (Rotation:none)",xlim=xrange,ylim=yrange)
 grid(col = "black")

 # xrange = range(var_fa$loadings[,1])
 # yrange = range(var_fa$loadings[,2])
 plot(loadings(var_fa),pch=19,col='blue',xlab="Loadings for Factor 1", ylab="Loadings for Factor 2",   main="Factor Loadings (Rotation:varimax)",xlim=xrange,ylim=yrange)
 grid(col = "black")

 # xrange = range(pro_fa$loadings[,1])
 # yrange = range(pro_fa$loadings[,2])
 plot(loadings(pro_fa),pch=19,col='blue',xlab="Loadings for Factor 1", ylab="Loadings for Factor 2",   main="Factor Loadings (Rotation:promax)",xlim=xrange,ylim=yrange)
grid(col = "black")
```

As evident from the above plots, rotations (varimax and promax) do make it slightly easier to notice groups than unrotated loadings. Grouping the variables, to some extent, is a subjective matter and based on the promax rotation plot I would say 3 groups is more appropriate than 2. 

We have already analyzed & interpreted the loadings for 2-factor solution above.  <br/>

In the following, we plot the factor scores for each rotation type.
```{r echo = FALSE,fig.align="center",fig.width=10,fig.height=4}
   par(mfrow=c(1,3))  
   # plot(no_fa$scores,pch=19,col="blue",xlab="Factor 1 Score",ylab="Factor2 Score")
   SA(no_fa$scores,main="Factor Scores (rotation:none)")
   SA(var_fa$scores,main="Factor Scores (rotation:varimax)")
   SA(pro_fa$scores, main="Factor Scores (rotation:promax)")
   grid(col="black")
```

The normalized scores scatter plots above don't show any extreme outliers ($>3*\sigma \ or < -3*sigma$). The scores also show normality in that most of the data is concerntrated inside the smaller circles.

Below we show the residual matrices for each rotation type.
```{r echo = FALSE,results='asis'}
    res_no <- round(res_no,3)
    res_var <- round(res_var,3)
    res_pro <- round(res_pro,3)
    "$$ Residual_{norotation} = " %_% res_no %_% "$$"
    "$$ Residual_{varimax} = " %_% res_var %_% "$$"
    "$$ Residual_{promax} = " %_% res_pro %_% "$$"
```
The residual matrix for no rotation and varimax rotation are the same. 'promax' rotation seems to yeild the highest values in the residual matrix. It's also worth noting that the diagonal entires for no-rotation and varimax rotation are 0 while for 'promax' rotation they are not. We want very low values in residual matrices, ideally zeros. 

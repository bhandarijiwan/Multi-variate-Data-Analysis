---
output:
  html_document: default
  pdf_document: default
---
<!-- ---
title: 'Statistical Lab 3 (Transformation to Normality and PCA R) '
author: "Jiwan Bhandari"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: no
vignette: |
%\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---
-->

<div style="text-align:center;min-height:29.7cm;width:21cm;vertical-align:center;">
<h4 style="">STAT 755 Lab Report 3</h4>
<h4>Jiwan Bhandari</h4>
</div>

```{r setup, include=FALSE}
library(car)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
```

#### Dataset
The dataset that we will be using for this exercise is called USArrests. This dataset come bundled with R installation. The data set contains data arrests for violent crime across all US states. There are four, almost self descriptive variables  - Murder, Assault, Rape and UrbanPop. Each variable is observed per 100,000 residents.

#### Transformation to Normality
Many statistical techniques assume normality of the data. The quality of results obtained from these techniques depend on how closely the population resembles normal distribution. But of course not all of the data is normally distributed. Thus, we need techniques that can help transform non-normal data to normal or near-normal. Transforming the data to near normal helps eliminate skewness and non-uniform variance. 

#### BoxCox Transformation
Power transformation is family of transformation that transform the data using power functions. BoxCox transformation is one power transformation technique that's fairly popular. BoxCox transformation is formlated as follows:
$$ x^\lambda= \bigg\{ 
\begin{array}
 1\frac{x^\lambda-1}{\lambda}\ \  \lambda≠0\\
 \ln{x} \ \ \ \lambda = 0
\end{array} $$

The parameter $\lambda$ is estimated using maximum likelihood estimation technique. Usually $\lambda$ is estimated using a computer rather than analytically by repeatedly trying different values of from $-2 \ to \ 2$ and selecting the best value that maximizes the likelihood profile.
Below we use R's $boxCox$ transformation function to try to transform each of the variables in $USArrests$ dataset. First, we estimate the optimal(best) $\lambda$ parameter seperately for each variable and then plot the likelihood profile for each parameter.
```{r echo = TRUE}
  lambda<-data.frame(row.names = 1)
  p<-apply(USArrests,2,function(x){
    lambda<<-cbind(lambda,powerTransform(x)$lambda)
  })
```
  
  The following table shows the best estimates for $\lambda s$ for each variable in the $USArrests$ dataset.
```{r echo=FALSE}  
  names(lambda)<-names(USArrests)
  rownames(lambda)<- "Lambda"
  knitr::kable(lambda)
```

```{r echo=FALSE , warning=FALSE,fig.align='center',fig.width=10,fig.height=6}
  par(mfrow=c(2,2))
  i<-1
  p<-apply(USArrests,2,function(x){
    boxCox(x~1,main=names(USArrests)[i])
    i<<-i+1
  })
```

Having figured out the $\lambda \ s$ for the variables, we can now go ahead and do the boxCox transformation. In the following, we transform the variables of $USArrests$ using the transformation equations stated above. After transforming the variables, we look at their normal quantiles plot to see if the transformation has approximated normal distribution. We plot two quantile plots for each variable side by side to see if the transformation helped us achieve normality.


```{r echo=TRUE,fig.align='center',fig.height=4,fig.width=10}
  i<-1
  n<-names(USArrests)
  par(mfrow=c(1,2))
  p<-apply(USArrests,2,function(x){
     qqnorm(x,main=paste(n[i], "Before Transformation"))
     qqline(x)
     grid()
     t<- bcPower(x,lambda = lambda[,i]) 
     qqnorm(t,main=paste(n[i], "After Transformation"))
     qqline(t)
     i<<-i+1
     grid()
 })
```



#### Principal Component Analysis 
Principal component analysis is a dimensionality reduction technique that can map data in higher dimension to lower dimension while preserving as much information as possible. Geometrically, principal component analysis is equivalent to change of basis. The new basis represent the directions with maximum variability and provide a simpler representation of the covariance structure.
Let the random vector $X =  [X_1,X_2,....X_p]$ have the covariance matrix $\Sigma$ with eigen value $\lambda_1≥\lambda_2≥.....≥\lambda_p≥0$. Let $\Sigma$ have the eigenvalue-eigen vector pars $(\lambda_1,e_1),(\lambda_1,e_2),.....,(\lambda_p,e_p$  The $i$th principal component of $X$, $Y_i$ = linear combination of $a_i'X$ that maximizes Var($a_i'X$) subject to $a_i'a_i=1$ and Cov($a_i'X,a_k'X$)=0 for $k<i$. 
$$Y_i = e_i'X =e_{i1}X_1+e_{i2}X_2+......+e_{ip}X_p,\ \ i=1,2,.....,p$$
In the following section we perform PCA on the USArresets dataset. We will then try to study several characteristics of the data based on the result of PCA.
First, we will look the variances of the variables in the dataset.

```{r echo=FALSE}
  knitr::kable(cov(USArrests))
```

As evident of the table above, the variances of the variables differ significantly, in order of magnitude, so we want to use correlation matrix instead of co-variance matrix in our PCA calculation.  

```{r echo = TRUE}
  p<-princomp(x=USArrests,cor = TRUE)
```
The coefficients of linear transformations, also called loadings, are shown below.
$$
L_1=
\left[
\begin{array}
  =`r round(p$loadings[,1],3)`
\end{array}
\right] \\
L_2=
\left[
\begin{array}
  `r round(p$loadings[,2],3)`
\end{array}
\right] \\
L_3=
\left[
\begin{array}
  `r round(p$loadings[,3],3)`
\end{array}
\right] \\
L_4=
\left[
\begin{array}
  `r round(p$loadings[,4],3)`
\end{array}
\right]
$$
The scree plot showing the variances of the principal components is shown below.
```{r echo=FALSE,fig.align='center'}
  plot(p)
```
From the scree plot, it's evident that the first components embodies a large chunk of the variance of the data. The second components also embodies significant variances while the remaining components embody the remaining variance information. Typically, when we do a PCA this is the pattern that's desired where few initial components embody as much variance as possible so that more and more information can be preserved within those components.

The variance of these components are nothing but the eigen values of $\Sigma$ of scaled USArrest dataset. The transformation coefficients, also known as bindings are the eigen vectors corresponding to $\Sigma$.  

Below, we calculate and formulate the actual principal components pertaining to USArrests dataset using the loadings and scaling parameter we have just calculated.

```{r echo=TRUE}
  ND<- as.matrix(USArrests)%*%diag(1/p$scale) # Normalize the data using the scale information
  Y<- ND%*%as.matrix(p$loadings)
```

```{r echo=FALSE}
  l1<-round(p$loadings[,1],3)
  l2<-round(p$loadings[,2],3)
  l3<-round(p$loadings[,3],3)
  l4<-round(p$loadings[,4],3)
  n<-names(USArrests)
```
$$ Y_1= (`r l1[1]`)*`r n[1]` + (`r l1[2]`)*`r n[2]` + (`r (l1[3])`)*`r n[3]` + (`r (l1[4])`)*`r n[4]`$$
$$ Y_2= (`r l2[1]`)*`r n[1]` + (`r l2[2]`)*`r n[2]` + (`r (l2[3])`)*`r n[3]` + (`r (l2[4])`)*`r n[4]`$$
$$ Y_3= (`r l3[1]`)*`r n[1]` + (`r l3[2]`)*`r n[2]` + (`r (l3[3])`)*`r n[3]` + (`r (l3[4])`)*`r n[4]`$$
$$ Y_4= (`r l4[1]`)*`r n[1]` + (`r l4[2]`)*`r n[2]` + (`r (l4[3])`)*`r n[3]` + (`r (l4[4])`)*`r n[4]`$$

Following table varifies that the principal components are independent.
```{r echo=FALSE}
    c<- as.data.frame(round(cov(Y),3))
    names(c)<-c("Y1","Y2","Y3","Y4")
    rownames(c)<-names(c)
    knitr::kable(c)
```

#### Correlations between principal components and original data

The correlation coefficients between the components $Y_i$ and the variables $X_k$ is given by the following equation
$\rho_{Y_{i},X_{k}}=\frac{e_{ik}\sqrt{\lambda_i}}{\sqrt{\sigma_{kk}}} \ \  i,k=1,2,....,p$

Using the above equation we calculate the correlation between the first principal component and the original variables of USArrest dataset.

```{r echo=TRUE}
   s <- cov(ND)
   e <- eigen(s)
   t <- 1/sqrt(s) * diag(4)
   r<- round((sqrt(e$values[1])*e$vectors[,1])%*%t,2)
```

The following vector $\rho_{Y_{1},X_{k}}$ shows the correlation coefficients between the first principal component $Y_1$ and the variables $X_k$.
$$\rho_{Y_{1},X_{k}}
=\left[\begin{array}
`r r`
\end{array}
\right]
$$

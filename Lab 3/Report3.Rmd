---
output:
  html_document: default
  pdf_document: default
---
<!-- ---
title: 'Statistical Lab 3 (Transformation to Normality and PCA R) '
author: "Jiwan Bhandari"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: no
vignette: |
%\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---
-->

<div style="text-align:center;min-height:29.7cm;width:21cm;vertical-align:center;">
<h4 style="">STAT 755 Lab Report 3</h4>
<h4>Jiwan Bhandari</h4>
</div>

```{r setup, include=FALSE}
library(car)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
```

#### Dataset
The dataset that we will be using for this exercise is called USArrests. This dataset come bundled with R installation. The data set contains data arrests for violent crime across all US states. There are four, almost self descriptive variables  - Murder, Assault, Rape and UrbanPop. Each variable is observed per 100,000 residents.

#### Transformation to Normality
Many statistical techniques assume normality of the data. The quality of results obtained from these techniques depend on how closely the population resembles normal distribution. But of course not all of the data is normally distributed. Thus, we need techniques that can help transform non-normal data to normal or near-normal. Transforming the data to near normal helps eliminate skewness and non-uniform variance. 

#### BoxCox Transformation
Power transformation is family of transformation that transform the data using power functions. BoxCox transformation is one power transformation technique that's fairly popular. BoxCox transformation is formlated as follows:
$$ x^\lambda= \bigg\{ 
\begin{array}
 1\frac{x^\lambda-1}{\lambda}\ \  \lambda≠0\\
 \ln{x} \ \ \ \lambda = 0
\end{array} $$

The parameter $\lambda$ is estimated using maximum likelihood estimation technique. Usually $\lambda$ is estimated using a computer rather than analytically by repeatedly trying different values of from $-2 \ to \ 2$ and selecting the best value that maximizes the likelihood profile.

Below we use R's $boxCox$ transformation function to try to transform each of the variables in $USArrests$ dataset. First, we estimate the optimal(best) $\lambda$ parameter seperately for each variable and then plot the likelihood profile for each parameter.
```{r echo = TRUE}
  lambda<-data.frame(row.names = 1)
  p<-apply(USArrests,2,function(x){
    lambda<<-cbind(lambda,powerTransform(x)$lambda)
  })
```
  
  The following table shows the best estimates for $\lambda s$ for each variable in the $USArrests$ dataset.
```{r echo=FALSE}  
  names(lambda)<-names(USArrests)
  rownames(lambda)<- "Lambda"
  knitr::kable(lambda)
```

```{r echo=FALSE , warning=FALSE,fig.align='center',fig.width=10,fig.height=6}
  par(mfrow=c(2,2))
  i<-1
  p<-apply(USArrests,2,function(x){
    boxCox(x~1,main=names(USArrests)[i])
    i<<-i+1
  })
```

Having figured out the $\lambda \ s$ for the variables, we can now go ahead and do the boxCox transformation. In the following, we transform the variables of $USArrests$ using the transformation equations stated above. After transforming the variables, we look at their normal quantiles plot to see if the transformation has approximated normal distribution. We plot two quantile plots for each variable side by side to see if the transformation helped us achieve normality.


```{r echo=TRUE,fig.align='center',fig.height=4,fig.width=10}
  i<-1
  n<-names(USArrests)
  par(mfrow=c(1,2))
  p<-apply(USArrests,2,function(x){
     qqnorm(x,main=paste(n[i], "Before Transformation"))
     qqline(x)
     grid()
     t<- bcPower(x,lambda = lambda[,i]) 
     qqnorm(t,main=paste(n[i], "After Transformation"))
     qqline(t)
     i<<-i+1
     grid()
 })
```
Inspecting before and after transformation quantile plots for each of the variables, we see that all most all variables don't deviate severly from normality even though they may not be perfectly normal. We observe that $Rape$ has a bit of right tail and transforming it does help get rid of the right tail. For other variables transformation doesn't have much effect so we can assume that the distribution of these variables is more or less symmetric.  Below we plot before and after transformation histograms of all variables. 
```{r echo=TRUE,fig.align='center',fig.height=4,fig.width=10}
 i<-1;
 par(mfrow=c(1,2))
 X<-apply(USArrests,2,function(x){
     hist(x,main=paste(n[i], "Before Transformation"))
     grid()
     t<- bcPower(x,lambda = lambda[,i]) 
     hist(t,main=paste(n[i], "After Transformation"))
     i<<-i+1
     grid()
     t
 })
```

Observing from the histograms, transformation does add more symmetry to the variables, especially $Rape.$ $Murder$ also appears more symmetric after transformation. Overall, transforming would be a good idea.

#### Principal Component Analysis 
Principal component analysis is a dimensionality reduction technique that can map data in higher dimension to lower dimension while preserving as much information as possible. Geometrically, principal component analysis is equivalent to change of basis. The new basis represent the directions with maximum variability and provide a simpler representation of the covariance structure.
Let the random vector $X =  [X_1,X_2,....X_p]$ have the covariance matrix $\Sigma$ with eigen value $\lambda_1≥\lambda_2≥.....≥\lambda_p≥0$. Let $\Sigma$ have the eigenvalue-eigen vector pars $(\lambda_1,e_1),(\lambda_1,e_2),.....,(\lambda_p,e_p$  The $i$th principal component of $X$, $Y_i$ = linear combination of $a_i'X$ that maximizes Var($a_i'X$) subject to $a_i'a_i=1$ and Cov($a_i'X,a_k'X$)=0 for $k<i$. 
$$Y_i = e_i'X =e_{i1}X_1+e_{i2}X_2+......+e_{ip}X_p,\ \ i=1,2,.....,p$$
In the following section we perform PCA on the USArresets dataset. We will then try to study several characteristics of the data based on the result of PCA.
First, we will look the variances of the variables in the dataset.

```{r echo=FALSE}
  knitr::kable(cov(X))
```

As evident of the table above, the variances of the variables differ significantly, in order of magnitude, so we want to use correlation matrix instead of co-variance matrix in our PCA calculation.  

```{r echo = TRUE}
  p<-princomp(x=X,cor = TRUE)
```
The coefficients of linear transformations, also called loadings, are shown below.
$$
L_1=
\left[
\begin{array}
  =`r round(p$loadings[,1],3)`
\end{array}
\right] \\
L_2=
\left[
\begin{array}
  `r round(p$loadings[,2],3)`
\end{array}
\right] \\
L_3=
\left[
\begin{array}
  `r round(p$loadings[,3],3)`
\end{array}
\right] \\
L_4=
\left[
\begin{array}
  `r round(p$loadings[,4],3)`
\end{array}
\right]
$$
The scree plot showing the variances of the principal components is shown below.
```{r echo=FALSE,fig.align='center'}
  plot(p)
```
From the scree plot, it's evident that the first components embodies a large chunk of the variance of the data. The second components also embodies significant variances while the remaining components embody the remaining variance information. Typically, when we do a PCA this is the pattern that's desired where few initial components embody as much variance as possible so that more and more information can be preserved within those components.

The variance of these components are nothing but the eigen values of $\Sigma$ of scaled USArrest dataset. The transformation coefficients, also known as bindings are the eigen vectors corresponding to $\Sigma$.  

Below, we calculate and formulate the actual principal components pertaining to USArrests dataset using the loadings and scaling parameter we have just calculated.

```{r echo=TRUE}
  ND<- as.matrix(X)%*%diag(1/p$scale) # Normalize the data using the scale information
  Y<- ND%*%as.matrix(p$loadings)
```

```{r echo=FALSE}
  l1<-round(p$loadings[,1],3)
  l2<-round(p$loadings[,2],3)
  l3<-round(p$loadings[,3],3)
  l4<-round(p$loadings[,4],3)
  n<-names(USArrests)
  
  
```
$$ Y_1= (`r l1[1]`)*`r n[1]` + (`r l1[2]`)*`r n[2]` + (`r (l1[3])`)*`r n[3]` + (`r (l1[4])`)*`r n[4]`$$
$$ Y_2= (`r l2[1]`)*`r n[1]` + (`r l2[2]`)*`r n[2]` + (`r (l2[3])`)*`r n[3]` + (`r (l2[4])`)*`r n[4]`$$
$$ Y_3= (`r l3[1]`)*`r n[1]` + (`r l3[2]`)*`r n[2]` + (`r (l3[3])`)*`r n[3]` + (`r (l3[4])`)*`r n[4]`$$
$$ Y_4= (`r l4[1]`)*`r n[1]` + (`r l4[2]`)*`r n[2]` + (`r (l4[3])`)*`r n[3]` + (`r (l4[4])`)*`r n[4]`$$

Following variance-covariance table for $Y$ varifies that the principal components are independent.
```{r echo=FALSE}
    c<- as.data.frame(round(cov(Y),3))
    names(c)<-c("Y1","Y2","Y3","Y4")
    rownames(c)<-names(c)
    knitr::kable(c)
```

#### Correlations between principal components and original data

The correlation coefficients between the components $Y_i$ and the variables $X_k$ is given by the following equation
$\rho_{Y_{i},X_{k}}=\frac{e_{ik}\sqrt{\lambda_i}}{\sqrt{\sigma_{kk}}} \ \  i,k=1,2,....,p$

Using the above equation we calculate the correlation between the first principal component and the original variables of USArrest dataset.

```{r echo=TRUE}
   s <- cov(ND)
   e <- eigen(s)
   t <- 1/sqrt(s) * diag(4)
   r <- round((sqrt(e$values[1])*e$vectors[,1])%*%t,2)
```

The following vector $\rho_{Y_{1},X_{k}}$ shows the correlation coefficients between the first principal component $Y_1$ and the variables $X_k$.
$$\rho_{Y_{1},X_{k}}
=\left[\begin{array}
\ 
`r abs(r)`
\end{array}
\right]
$$
The variable $X2 (Assault)$ with transformation coefficient $`r round(l1[2],3)`$ receives the greatest weight in component $Y_1$. It also has the largest correlation (in absolute value) with $Y_1$. The correlation of $Rape$ with $Y_1$ is `r r[4]` which is very close to $Murder$, indicating that the two variables  are almost equally significant to the the first principal component. We illustrate this further using the following plot where we plot each variable against the first principal componet.

```{r echo=FALSE,fig.align='center',fig.height=6}
par(mfrow=c(2,2))
i<-1
p<-apply(ND,2,function(x){
  plot(x,Y[,1],xlab=n[i],ylab="PC1",type='p',main = paste(n[i]," vs PC1"))
  i<<-i+1
})
```

#### Interpreting the principal components

So far, we have calculated the loadings, which are also the coefficients of linear transformation, using these loadings calculated the actual principal components. We also calcualted the correlation of the first principal component with all the variables. We used a scaled/standarized dataset i.e we used Correlation matrix $\rho$ to calculate the loadings instead of $\Sigma$ because the variances of variables were very different. Using $\rho$ to calculate the loading helped us see the influence of each variables in each principal component instead of having variable with high variance overwhelm the principal components. 
<br/>
The first principal component $Y_1$ is mixture of all variables and from the coefficients (weights) of transformation it appear all variables have importance in $Y_1$ even though $Assault,\ Rape \ \& \ Murder$ have more relative importance than $Urbanpop$. Moreover, this principal component explains $\frac{\lambda_1}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}=$ `r round(e$value[1]/sum(e$values),2)` of the total variance. It's safe to infer that $Y_1$ capture more of the variances in crime than urbanpopulation. The second princpal component is completely dominated by $UrbanPop$ and ratio of it's weight to weight of other variables is easily greater 2. So, we can safely say that second principal component mostly captures the variances in UrbanPopulation. The second principal component explains $\frac{\lambda_2}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}=$ `r round(e$value[2]/sum(e$values),3)` of the total variance. The third principal component is heavely dominated by the variable $Rape$ and this component captures $\frac{\lambda_3}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}=$ `r round(e$value[3]/sum(e$values),3)` of the total variance. The fourth caputres $\frac{\lambda_4}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}=$ `r round(e$value[4]/sum(e$values),3)` of the total variance.
<br/><br/>
#### Extra Credit

##### Normality

Let's suppose $X$ is a normally distributed random variable. $X\sim N(\mu,\sigma).$ Let's suppose $Y$ & $Z$ be two random variable defined as $Y=X^4$ and $Z=\sqrt[4]{y}$. Analytically, $X=Z$ so $Z$ should have same distribution as $X$ but in practice and in computation $Z≠N(\mu,\sigma)$. The reason for this is that $X$ could be a vector of both negative and positive number but $Y=X^4,$ is a vector of only positive numbers. Likewise, $Z=\sqrt[4]{Y}$, is also a vector of only positive numbers. So, in practice $Z≠X$ even though analytically it makes sense. Since $Z≠X$, $Z$ doesn't have the same distribution as $X$ and $Z's$ distribution isn't normal because the it will be skewed due to negative numbers being changed to positive.

##### BoxCox Transformation
```{r echo=TRUE,fig.align='center',fig.width=10,fig.height=6}
  X<-rnorm(1000)
  Y<-X^4  # Y is non-normal
  Z<-X^3  # Z is non-normal
  Z<- Z-min(Z)+sd(Z)
  Z_T <- Z
  Y_T <- Y
  par(mfrow=c(2,2))
  qqnorm(Y_T,main="Before Transfrom X^4")
  qqline(Y_T)
  qqnorm(Z_T,main="Before Transfrom X^3")
  qqline(Z_T)
  Y_T <- bcPower(Y_T,lambda = powerTransform(Y_T)$lambda)
  Z_T <- bcPower(Z_T,lambda = powerTransform(Z_T)$lambda)
  qqnorm(Y_T,main="After Transformation X^4")
  qqline(Y_T)
  qqnorm(Z_T,main="After Transformation X^3")
  qqline(Z_T)
```
As we can see from the plots above $X^4$ can be transformed to near-normal with BoxCox transfromation while $X^3$ can't be transformed to near normal. The reason for this is that boxcox transformation isn't very ideal for transforming already symmetric distribution. As it happens, $X^4$ is and skewed distribution while $X^3$ is pretty symmetric. This will be more clear from the following histograms. 

```{r echo=FALSE,fig.align='center',fig.width=10,fig.height=4}
  par(mfrow=c(1,2))
  hist(Y,main="Histogram for X^4")
  hist(Z,main="Histogram for X^3")
```

In general, BoxCox transformation doesn't work very well transforming from already symmetric distributions. Below we illustrate this point further by trying to transform from Uniform distribution which is a symmetric continuous distribution.
```{r echo=TRUE,fig.align='center',fig.width=10,fig.height=6}
  X<-runif(1000)
  par(mfrow=c(2,2))
  hist(X,main="Before transformation (uniform X) ")
  qqnorm(X,main="Before transformation (uniform X)")
  qqline(X)
  X_T <- bcPower(X,lambda = powerTransform(X)$lambda)
  hist(X_T,main="After transformation (uniform X)")
  qqnorm(X_T,main="After transformation (uniform X)")
  qqline(X_T)
```


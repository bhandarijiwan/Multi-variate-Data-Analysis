---
output:
  html_document: default
  pdf_document: default
---
<!-- ---
title: 'Statistical Lab 3 (Transformation to Normality and PCA R) '
author: "Jiwan Bhandari"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: no
vignette: |
%\VignetteIndexEntry{Vignette Title} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---
-->

<div style="text-align:center;min-height:29.7cm;width:21cm;vertical-align:center;">
<h4 style="">STAT 755 Lab Report 4</h4>
<h4>Jiwan Bhandari</h4/><br>
</div>

```{r setup, include=FALSE}
library(car)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(GGally)
library(dendextend)
library(colorspace)
library(knitr)
data("iris")
```
<br/>

#### Dataset
The dataset used in this exercise is called *iris*. This dataset comes bundled with R. The data set contains 4 quantitative variables which are the measurements in centimeters of the variables sepal length, sepal width, petal length and petal width. There is also 1 ordinal variable - species. The levels of species are *iris setosa*, *versicolor* , and *virginica*. There are 150 observations.

#### Plotting the *Iris* dataset

```{r echo=FALSE,fig.align='center',fig.width=10,fig.height=8}
  ggpairs(iris,columns = 1:4,aes(colour=Species,shape=Species),diag=list(continuous="densityDiag"),title = "Iris Dataset",legend=c(4,1))+theme(
    legend.position = "bottom",plot.title = element_text(hjust=0.5))
  
```

The above plot shows a matrix of plots. Plots below the diagonal show the scatter plot between each of the variables of iris dataset. Plots along the diagonal show the density distribution across each species. Plots above the diagonal show the correlation between each variable, also the correlation among these variables by species.


#### Observation 
The most general observation from the plot is that species $setosa$ shows very clear seperability from other species. $Versicolor\  \&\ Virginica$ seem cluttered together hence more difficult to seperate into different clusters. 

Based on sepal length and sepal width, we have clear seperation of species $setosa$ but the observations for the other two species are intermixed. These two variable pairs don't seem like very good choice for clustering the species.

Apart from sepal length vs sepal width, most of the plots, for most part, tell the same story. $Setosa$ is clearly distinct from the other two sepecies. $Versicolor$ and $Virginica$ aren't as distinct and far apart as $setosa$ but more or less seperable. Of all these plots petal length vs petal width seems most promising.

Based on petal length and petal width plot, we see that $setosa$, $versicolor$, & and $virginica$ have increasing petal length and petal width and in that order. We also have the highest correlation among these two variables of all the other variable pairs. Another important observation from the plot is that the intra-cluster distance among the species looks minimal of all the variable pairs.


#### Hierarchical Clustering

Hierarchical clustering technique for creating clusters that have a predetermined ordering from either bottom up (Agglomerative) or top to bottom (Divisive). For the purpose of this report, we are intersted in agglomerative hierarchical clustering technique which has the following general steps (from textbook).<br/>
1. Start with N clusters, each containing a single variable and an $N * N$ symmetric matrix of distances **D**.<br/>
2. Search the distance matrix fro the nearest (most similar) pair of clusters. Let the distance between "most similar" clusters $U$ and $V$ be $d_{UV}$.<br/>
3. Merge clusters $U$ and $V$. Label the newly formed cluster $(UV)$. Update the entries in the distance matrix by deleting the entries for merged variables and appending distance entries for newly formed cluster $(UV)$. <br/>
4. Repeat steps 2 and 3, $N-1$ times. <br/>

In the following, we performing hierarchical clustering in the $iris$ dataset using several distance metrics $(euclidean,\ manhatten,\ minkowski)$ and several linkage methods $(Single\ Linkage, \ Complete\ Linkage, \ Average\ Linkage).$ We will only use petal.length and petal.width because these two variables showed the most sperability. 

##### Single Linkage
In single linkage groups are formed from the individual entities by merging nearest neighbour, where the term *nearest neighbor* connotes the smallest distance.
```{r echo=FALSE}
    distances <- c("manhattan","euclidean","minkowski");
    shortSpecies <- substr(iris$Species,start=1,stop = 2)
    methods <- c("single","complete","average")
```

```{r echo=FALSE,fig.align='center',fig.width=10,warning=FALSE}
iris2<- iris[,3:4]
m <- methods[1]
d_iris <- dist(iris2,method=distances[1])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-.01,main=paste("(distance =",distances[1],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable<-as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

  
d_iris <- dist(iris2,method=distances[2])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[2],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

d_iris <- dist(iris2,method=distances[3])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[3],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

```

When using 'single' linkage method, as shown by the preceeding plots and summary table, we get  very poor clustering behaviour across $versicolor$ & $virginica$. Cluster 3 contained only 1 or 2 samples and the behaviour is the same for all three distances. So, it would be safe to say that 'single' linkage method isn't very well suited for clustering the species.

##### Complete Linkage
In complete linkage, the link between two clusters contains all element pairs, and the distances between clusters equals the distance between those two elements that are farthest away from each other. Complete linkage solves a drawback of single linkage method where clusters fromed via single linkage clustering may be forced together due to single element being close to each other, even though many of the elemnets in each cluster may be very distant to each other. Complete linkage tends to find compact clusters of approximately equal diameters.

```{r echo=FALSE,fig.align='center',fig.width=10,warning=FALSE}
iris2<- iris[,3:4]
m <- methods[2]
d_iris <- dist(iris2,method=distances[1])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-.01,main=paste("(distance =",distances[1],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable<-as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

  
d_iris <- dist(iris2,method=distances[2])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[2],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

d_iris <- dist(iris2,method=distances[3])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[3],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(dend,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

```

When using complete linkage for clustering, as shown by the above plots and summary table, except for cluster 1, we get different clustering performance across different distance measures. 
Using 'manhattan' distance, cluster 1 contains only $setosa$ but cluster 2 & 3 are a mixture of $versicolor$ and $virginica$. In terms of proportion of correctly clustered samples, we have $\sim 96\%$ of the samples placed in correct clusters. 

Using 'euclidean' and 'minkowski' distances, we have around $\sim 86\%$ identification accuracy.

##### Average Linkage
In average linkage clustering, the distance between two clusters is defined as the average of the distances between all pairs of objects, where each pair is made up of one of object from each group.

```{r echo=FALSE,fig.align='center',fig.width=10,warning=FALSE}
iris2<- iris[,3:4]
m <- methods[3]
d_iris <- dist(iris2,method=distances[1])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-.01,main=paste("(distance =",distances[1],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable<-as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

  
d_iris <- dist(iris2,method=distances[2])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[2],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

d_iris <- dist(iris2,method=distances[3])
hc_iris  <- hclust(d_iris,method=m)
dend <- as.dendrogram(hc_iris)
# # Color the branches based on the clusters
dend <- color_branches(dend, k=3,groupLabels = c("1","3","2"))
dend <- set(dend,"labels_cex",0.5)
plot(dend,hang=-0.01,main=paste("(distance =",distances[3],")",sep=""),nodePar = list(cex=0.009))
c <-cutree(hc_iris,k=3)
resultTable <- as.data.frame.matrix(table(c,iris$Species))
rownames(resultTable)<-c("Cluster 1","Cluster 2","Cluster 3")
knitr::kable(resultTable)

```

As with 'single' and 'complete' linkage methods, with 'average' linkage species $setosa$ has been  clustered distinctly over all distance measures.

For each distance measures - 'manhattan', 'euclidean', 'minkowski', we have around $\sim$ 96% identification accuracy. Cluster 2 which is primarily cluster of $versicolor$ has 1 sample from $virginica$. Cluster 3, which is primarily cluster of $virginica$ species has 5 samples from $versicolor$ species. 

##### Best Parameters and Clustering Results

After trying three linkage methods - 'single', 'complete', 'average'  with three distance measures - 'manhattan','complete','average'  to cluster on the species, we got the best identification accuracy of $\sim 96\%$ (144 correctly clustered samples) using 'complete' linkage method with 'manhattan' distance measure and using 'average' linkage method with any distance of the above three distance measures. 


##### Canonical Correlation

Canonical correlation analysis focuses on the correlation between a linear combination of the variables in one set and a linear combination of the variables in another set. In canonical correlation, we measure the association between two groups of variables. Let *$X^1$* be the random vector representing the first group of variables and let *$X^2$* be the second group of variables. 

In canonical correlation, we map the preceeding vectors *$X^1$* and  *$X^2$* to new variables $U$ and $V$ by linearly combining the components of the vectors, specifically,
$$U=a'X^{(1)}\\V=b'X^{(2)}$$
for some pair of coefficients vectors *a* and *b*. The goal of the canonical correlation is to seek the pair coefficient vector *a* and *b* that maximize the $$Cor(U,V) =\frac{a'\Sigma_{12}b}{\sqrt{a'\Sigma_{11}a}\sqrt{b'\Sigma_{22}b}}$$

The *kth pair of canonical variables or kth canonical varitae pair*, is the pair of the linear combinations $U_k$, $V_k$ having variances, which maximize above correalation among all choices of uncorrelated with the previous $k-1$ canonical variable pairs.

##### Dataset
In the following, we perform canonical correlation analysis on a stock price data set. The dataset consists of 10 variables which are the stock prices for several companies. The dataset is a timeseries dataset of stock prices so it's desirable to apply some filtering to get rid of long term trends. 

The goal of this or series canonical correalation analysis is to find best two group the 10 variables into non-overlapping two groups that maximize the first canonical variate pairs.

```{r echo=FALSE}
   T<- read.table('Lab3_close.csv',sep=',',header = TRUE)  
   len <- dim(T)[1]
   Itime <- seq(len,1,by=-1)
   time <- 1989+(21+28+31+30+4)/365.25 + seq(1,len)/2250
   P<-T[,seq(2,11)]
   P<- log10(P)
   PD <- P*0
   for(i in seq(1,10)){
     t<-ts(P[,i],frequency = 365)
     s<- stl(t,s.window = 250)
     PD[,i]=t-s$time.series[,2]
   }
```

In the following table we tabulate several groups and their corresponding first cannonical correlation.
```{r echo=FALSE,warning=FALSE}
   n<-names(PD)
   x <- list(c(4,5,7,9),c(1,2,3,4,5),c(1,2,3,4,6),c(1,2,3,4,8,6),c(4,8,6,10),c(1,2,3,5),c(1,4,5,6,7))#
   y <- list(c(1,2,3,6,8,10),c(6,7,8,9,10),c(5,9,7,8,10),c(5,9,7,10),c(5,9,7,1),c(4,6,7,8),c(3,2,8,9,10)) #c(6,7,8,9,10)
   df <- data.frame(row.names = c("1","2","3"))
   i<-1
  for(v in x){
    X <- PD[v]
     Y <- PD[y[[i]]]
     vx<-toString(n[v],sep='')
     vy<-toString(n[y[[i]]],sep='')
     i <<-i+1
     cc <- toString(round(cancor(X,Y)$cor[1],3))
     df<- rbind(df,data.frame(vx,vy,cc))
  }
  names(df)<-c("Group1","Group 2","Correlation")
  kable(df)
```

The preceeding table shows the results of several grouping attempts and corresponding canonical correlation. From the table, we were able to get the maximum canonical correlation for first variate pair by grouping the variables as $(BA.close,\ NASDAQ.close,\ COCA.close,\ DELL.close,\ MD.close,\ GM.close )$ and $(GE.close,\ MSFT.close,\ IBM.close,\ PEPSI.close)$.
